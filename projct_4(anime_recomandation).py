# -*- coding: utf-8 -*-
"""ProJct-4(anime recomandation).ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1xwXcHHXqPAwfUUfceLORhuWDXpNAs0f1

# 데이터 선택이유
해당데이터는 타겟피쳐가 따로 정의되어 있지 않은 데이터셋으로, 현업에서 사용자들이 무엇을 선호할지 알지못하고 사용자들의 선호도를 예측해야하는 상황에서 도움을 줄 수 있는 경험이 될 것이라 생각했습니다. 또한 애니는 개인의 선호도를 명확하게 구분지을 수 있는 데이터라 생각했습니다.
추가적으로 애니 추천과 비슷하게 어떠한 물품이나, 서비스, 등과 같은 사용자의 선호도를 예측하여 추천을 해야하는 직무에 도움이 될것이라 생각하여 선택하게 되었습니다.

# 데이터 정보
- 출처: https://www.kaggle.com/hernan4444/anime-recommendation-database-2020?select=watching_status.csv

## 소개
17.562개의 애니에 대한 정보와 325.772명의 다른 사용자가 선호하는 정보가 포함되어 있습니다.

## 특이점
이 데이터 세트에는 성인용 애니(헨타이)에 대한 정보가 포함되어 있습니다.

## 콘텐츠
### animelist.csv
- 사용자가 각 점수와 함께 등록한 모든 애니 목록, 시청 현황 및 시청 회수
- 사용자별 애니 목록입니다. 중단, 완료, 시청 계획, 현재 시청 및 보류를 포함합니다.
  - user_id: 사용자 ID
  - anime_id: 애니의 ID
  - score: 사용자가 부여한 1에서 10 사이의 점수. 사용자가 점수를 할당하지 않은 경우 0
  - watch_status: 사용자의 애니 목록에 있는 이 애니의 상태.
  - Watched_episodes: 사용자가 본 에피소드 수.

### watching_status.csv
- animelist.csv의 watching_status에 해당하는 시청상태를 설명
  - status: 상태분류
  - description: 상태내용

### rating_complete.csv
- 사용자가 watching_status==2(완료)를 가진 애니에 부여한 등급 목록을 포함하는 CSV
- 사용자가 완전히 본 애니에 대해 부여한 등급입니다.
  - user_id: 사용자 ID
  - anime_id: 사용자가 평가한 애니 ID
  - rating: 사용자가 할당한 등급입니다.

### anime_with_synopsis.csv
- 장르 열에 Hentai가 포함되지 않은 애니 개요가 포함된 CSV
  - sypnopsis: 개요
  - 이외 칼럼은 anime.csv 파일과 중복됨

### anime.csv 
- 모든 애니에 대한 일반 정보를 포함합니다.
- 장르, 통계, 스튜디오 등과 같은 애니에 대한 정보
  - MAL_ID: 애니의 ID
  - Name: 애니의 전체 이름
  - Score: 모든 사용자가 제공한 애니의 평균 점수
  - Genres: 애니의 쉼표로 구분된 장르 목록 (예: 액션, 어드벤처, 코미디, 드라마, 공상과학, 우주)
  - English name: 애니의 영어로 된 전체 이름
  - Japanese name : 애니의 일본어로 된 전체 이름
  - Type': TV, 영화, OVA 등
  - Episodes': 챕터 수.
  - Aired : 방송일. 
  - Premiered: 시즌 초연. 
  - Producers: 쉼표로 구분된 제작자 목록
  - Licensors: 쉼표로 구분된 라이선스 제공자 
  - Studios: 쉼표로 구분된 스튜디오 목록
  - Source: 출처(만화, 라이트 노벨, 책 등)
  - Duration: 에피소드당 애니 길이
  - Rating: 연령등급(예: R - 17+(폭력 및 욕설))
  - Ranked: 점수에 따른 순위. (예: 28)
  - Popularity: 목록에 애니를 추가한 사용자 수에 따른 순위
  - Members: 이 애니의 "그룹"에 있는 커뮤니티 회원 수 (예: 1251960)
  - Favorites: 애니를 "즐겨찾기"로 보유한 사용자 수. (예: 61,971)
  - Watching: 애니를 보고 있는 사용자 수입니다. (예: 105808)
  - Completed: 애니를 완료한 사용자 수입니다. (예: 718161)
  - On-Hold: 애니를 보류 중인 사용자 수입니다. (예: 71513)
  - Dropped: 애니를 삭제한 사용자 수입니다. (예: 26678)
  - Plan to Watch': 애니를 시청할 계획인 사용자 수. (예: 329800)
  - Score-10': 10을 기록한 사용자 수(예: 229170)
  - Score-9': 9를 기록한 사용자 수(예: 182126)
  - Score-8': 8을 기록한 사용자 수(예: 131625)
  - Score-7': 7을 기록한 사용자 수(예: 62330)
  - Score-6': 6을 기록한 사용자 수(예: 20688)
  - Score-5': 5점을 받은 사용자 수(예: 8904)
  - Score-4': 4점을 받은 사용자 수(예: 3184)
  - Score-3': 3을 기록한 사용자 수(예: 1357)
  - Score-2': 2를 기록한 사용자 수(예: 741)
  - Score-1': 1을 기록한 사용자 수(예: 1580)

## 확인해볼만한 사항
### 데이터 확인 및 분석
- Popularity를 기준으로 상위 데이터를 나누고, 범주형 데이터들을 시각화를 해서 데이터를 분석함

### 가설
- 사용자들이 비슷한 내용을 좋아한다면, 시놉시스를 통해 비슷한 내용을 갖는 애니를 추천하면 좋을것이다.

- 상위데이터들의 시각화자료들을 분석하여 도출된 결과들은 모델로 예측한 값과 비슷할 것이다.

# 데이터 확인 및 분석
csv형식의 데이터를 불러와 테이블형으로 바꿔주고, 해당 데이터를 어떻게 사용할지 확인해본다.

## 데이터 불러오기, 데이터 확인, 약간의 전처리
"""

data_folder = "/Users/daenymac/Desktop/personal-project/ProJct-4(애니메이션 추천)/AI_02_이대운" # 파일 저장 위치
# 파일 저장 위치 + 파일명
data1 = data_folder+"/anime.csv"
data2 = data_folder+"/anime_with_synopsis.csv"
data3 = data_folder+"/animelist.csv"
data4 = data_folder+"/rating_complete.csv"
data5 = data_folder+"/watching_status.csv"

import numpy as np
import pandas as pd

# Name을 기준으로 병합이 가능, 하지만 시놉시스 하나만 병합하면 되는데
# 추천에서는 자연어처리를 통해서 사용해야 해서, 아직은 병합하지 않음 
# 추후 자연어처리로 새로운 인사이트가 창출 될 수 있는 지 확인 후 사용
df_anime = pd.read_csv(data1)
df_anime_with_synopsis = pd.read_csv(data2)

# df_rating_complete은 df_animelist에 종속된다 = 병합필요가 없다
# 사용할 가치가 있다면 사용하면되지 굳이 사용할 필요는 없다.
df_animelist = pd.read_csv(data3)
df_rating_complete = pd.read_csv(data4)

# df_animelist의 watching_status를 뉴메릭하게 바꾼 기준
df_watching_status = pd.read_csv(data5)

# 따라서 사용할만한 df는 df_anime, df_animelist 이다.

df_anime.dtypes

df_animelist.dtypes

df_anime.columns

df_animelist.head()

sum(df_anime.isna().sum()), sum(df_animelist.isna().sum()) # 결측치 없음

"""### 클리닝

#### 중복값 확인
"""

# print(sum(df_anime.duplicated()), sum(df_animelist.duplicated())) # df_animelist 중복값 존재
df_animelist = df_animelist.drop_duplicates() # 증복값 제거

# df_anime[df_anime['Name']=='Youkoso! Ecolo Shima'] 2
df_anime = df_anime.drop(15379) # 데이터 상 같은 애니같아서, popularity 가 낮은 로우 삭제

# df_anime[df_anime['Name']=='Hinamatsuri'] 2
df_anime['Name'][12826] = 'Hinamatsuri OVA' # 소스가 달라서 이름에 소스를 추가함
df_anime['Name'][13540] = 'Hinamatsuri TV'

# df_anime[df_anime['Name']=='Maou Gakuin no Futekigousha: Shijou Saikyou no Maou no Shiso, Tensei shite Shison-tachi no Gakkou e'] 3
df_anime = df_anime.drop([17543, 17544]) # 데이터 상 같은 애니같아서, popularity 가 낮은 로우 삭제

"""#### 카디널리티 줄이기"""

#Type: Music(1469)
index = df_anime[df_anime['Type']=='Music'].index
df_anime = df_anime.drop(index)

# df_anime['Aired']의 요소들을 첫 방영 년도로 수정하기
def year(i):
    if i == 'Unknown' or len(i) == 4:
        return i
    else:
        i = [a for a in i.split('to')][:1]
        if len(i[0]) > 5:
            i = [a for a in str(i[0]).split(',')][1].strip()
            return i
        else:
            i = i[0].strip()
            return i
    
df_anime['Aired'] = df_anime['Aired'].apply(year)

"""#### 피처 엔지니어링"""

score_col = ['Score-10','Score-9','Score-8','Score-7','Score-6','Score-5','Score-4','Score-3','Score-2','Score-1']
df_anime_score = df_anime[score_col] # 점수부분은 8점이상을 준 사용자의 비율로 축소 예정

def over8(df):
    df = df.replace('Unknown',0)
    df = df.astype(float)
    over_8 = []
    for i in range(0, len(df)):
        score = df.iloc[i]
        if 'Unknown' in score:
            over_8.append('Unknown')
        else:
            over_8.append(round((score[['Score-10','Score-9','Score-8']].sum()/score.sum())*100,1))
    return over_8

# 8점이상을 준 사용자들의 비율
df_anime['over8'] = over8(df_anime_score)

"""#### 피쳐 셀렉션"""

def col_del(df, del_ls):
    df1 = df.drop(del_ls, axis=1)
    return df1
# 불필요하다고 생각드는 피쳐 제거
del_col = ["Premiered","English name","Japanese name","Episodes"] + score_col
df_anime = col_del(df_anime,del_col)

df_anime.head()

"""## [상위1000, 상위5000, 전체] 데이터분석(Popularity 기준)"""

# 상위 1000개, 5000개 데이터 분류
top1000 = df_anime[(df_anime['Popularity']<1000) & df_anime['Popularity']>0]
top5000 = df_anime[(df_anime['Popularity']<5000) & df_anime['Popularity']>0]

"""### Rating"""

def count_rate_dic(df, col):
    counting_dict = dict(df[col].value_counts())
    return counting_dict

# 데이터별 장르 개수 확인
Rating1k = count_rate_dic(top1000, 'Rating')
Rating5k = count_rate_dic(top5000, 'Rating')
Ratingall = count_rate_dic(df_anime, 'Rating')

# 데이터 별 상위5개 장르
top10Rt_1k = sorted(Rating1k, key=Rating1k.get, reverse=True)[:3]
top10Rt_5k = sorted(Rating5k, key=Rating5k.get, reverse=True)[:3]
top10Rt_all = sorted(Ratingall, key=Ratingall.get, reverse=True)[:3]

# import matplotlib.pyplot as plt
# fig = plt.pie(ratio, labels=labels, autopct='%.1f%%', startangle=260, counterclock=False, shadow=True)
# f = fig.subplots(1, 3, sharey=False, squeeze=False)

# plt.suptitle('anime Studios')
# f[0,0].pie(top10Rt_1k, [Rating1k[i] for i in top10Rt_1k], autopct='%.1f%%')
# f[0,1].pie(top10Rt_5k, [Rating5k[i] for i in top10Rt_5k], autopct='%.1f%%')
# f[0,2].pie(top10Rt_all, [Ratingall[i] for i in top10Rt_all], autopct='%.1f%%')
# plt.show()

# 데이터 별 상위 10개 장르 시각화
import matplotlib.pyplot as plt
fig = plt.figure(constrained_layout=True)
f = fig.subplots(3, 1, sharey=False, squeeze=False)

plt.suptitle('anime Rating')
f[0,0].bar(top10Rt_1k, [Rating1k[i] for i in top10Rt_1k])
f[1,0].bar(top10Rt_5k, [Rating5k[i] for i in top10Rt_5k])
f[2,0].bar(top10Rt_all, [Ratingall[i] for i in top10Rt_all])
plt.show()

"""#### Rating 시각화 분석 
Rating의 분류는 아래의 등급으로 분류된다
- 등급
    - 'PG-13 - Teens 13 or older': 13세 이상 관람가
    - 'G - All Ages': 전체관람가
    - 'Rx - Hentai': 성인관람가(포르노 애니)
    - 'R - 17+ (violence & profanity)': 17세 이상 관람가(폭령성, 비속성)
    - 'R+ - Mild Nudity': 17세 이상 관람가(약간의 노출)
    - 'PG - Children':  부모지도하 어린이관람가


- 첫째,['PG-13 - Teens 13 or older'] 가 상위데이터 뿐만 아니라 전체데이터에서도 가장 많이 보입니다. 총 애니 중 절반이상이 해당 레이팅을 가지고 있는 것으로 보아 레이팅을 중심으로 추천하는 것은 적절하지 않지만, 이 레이팅에 해당하는 애니를 추천하면 높은 확률로 사용자가 선호할 수 있음을 의미합니다.

- 둘째, 1k,5k에서는 레이팅의 상위 3개가 동일하지만, 전체데이터에서 나타나는 레이팅에서는 상이한것을 통해 만18세 이상에 해당하는 애니는 연령제한으로 인해 접근이 용이하지 않아서 그런것으로 해석될 여지가 있으며, 연령제한에 걸리지 않는 사용자 중 해당 레이팅을 가진 애니에 좋은 점수 혹은 시청을 완료한것과 같은 정보가 존재한다면 해당 레이팅을 가진 애니를 추천했을때 사용자가 선호할 확률이 긍정적일 수 있다는 것으로 사료된다.

### Source
"""

# 데이터별 장르 개수 확인
Source1k = count_rate_dic(top1000, 'Source')
Source5k = count_rate_dic(top5000, 'Source')
Sourceall = count_rate_dic(df_anime, 'Source')

# 데이터 별 상위5개 장르
top10Sr_1k = sorted(Source1k, key=Source1k.get, reverse=True)[:5]
top10Sr_5k = sorted(Source5k, key=Source5k.get, reverse=True)[:5]
top10Sr_all = sorted(Sourceall, key=Sourceall.get, reverse=True)[:6]

fig = plt.figure(constrained_layout=True)
f = fig.subplots(3, 1, sharey=False, squeeze=False)

plt.suptitle('anime Source')
f[0,0].bar(top10Sr_1k, [Source1k[i] for i in top10Sr_1k])
f[1,0].bar(top10Sr_5k, [Source5k[i] for i in top10Sr_5k])
f[2,0].bar(top10Sr_all, [Sourceall[i] for i in top10Sr_all])
plt.show()

"""#### Source 시각화 분석
Source는 애니의 출처를 의미한다. 그 종류 중 그래프에 나타난 출처들은 다음과 같으며,
- 출처종류
    - Manga: 만화책
    - Light novel: 단편소설
    - Original: 애니
    - Visual novel:  영상 또는 이미지가 보이는 이야기 또는 일러스트를 내세운 소설
    - 4-koma manga: 4컷 만화
    - Game: 게임
    
그래프의 분석은 아래와 같다
- 첫째 애니가 원작인 애니가 제작량은 가장 많지만, 만화책을 원작으로 하는 애니가 상위 데이터에서 더 높은 순위를 차지하고 있는것을 확인할 수 있으며, 이를 통해 만화책을 원작으로 하는 애니를 사용자들이 더 선호함을 확인할 수 있다.

- 둘째 단편소설을 원작으로 하는 애니는 6번째 제작량에 위치하는것과 상위 데이터에서 2,3번째에 해당하는 것으로보아 제작량 대비 흥행하는 퍼센트가 높아서 사용자들에게 추천하면 긍정적인 반응이 올 것이라 분석할 수 있다.

### Type
"""

# 데이터별 장르 개수 확인
Type1k = count_rate_dic(top1000, 'Type')
Type5k = count_rate_dic(top5000, 'Type')
Typeall = count_rate_dic(df_anime, 'Type')

# 데이터 별 상위5개 장르
top10Tp_1k = sorted(Type1k, key=Type1k.get, reverse=True)
top10Tp_5k = sorted(Type5k, key=Type5k.get, reverse=True)
top10Tp_all = sorted(Typeall, key=Typeall.get, reverse=True)

import matplotlib.pyplot as plt
fig = plt.figure(constrained_layout=True)
f = fig.subplots(3, 1, sharey=False, squeeze=False)

plt.suptitle('anime Rating')
f[0,0].bar(top10Tp_1k, [Type1k[i] for i in top10Tp_1k])
f[1,0].bar(top10Tp_5k, [Type5k[i] for i in top10Tp_5k])
f[2,0].bar(top10Tp_all, [Typeall[i] for i in top10Tp_all])
plt.show()

"""#### Type 시각화 분석
Type의 종류는 5개로 아래와 같으며,
- 종류
    - TV: TV방영목적 시리즈형태
    - OVA: 오리지널 비디오 애니
    - Movie: 영화
    - Special: 극장판
    - ONA: 인터넷을 통해서만 공개되는 애니
    
그 순위를 데이터별로 분석하면 다음과 같다.

- 첫째 TV가 가장 많으며 상위 데이터셋에서도 가장 많은 비중을 차지하고 있다. 시리즈 형태의 애니로 한 회를 보기 시작하면 이후 회차까지 봐야해서 사용자들이 꾸준하게 이용이 가능하다는 특징을 통해 첫회가 긍정적인 추천이라는 생각이 들면 사용자들이 끝회차 까지 보게되는 장점이 존재한다.

- 둘째 1k에서 Movie의 순위와 다른 데이터셋에서 3번째에 존재한다는 점을 통해서 시리즈 형태가 아닌, 영화형태의 애니가 흔히 말하는 well-made인 경우, 적게 제작되더라도 10%이상의 작품들이 흥행에 성공할 수 있다는 분석이 가능하다 따라서 영화형태의 애니 중 사용자들에게 긍정적인 선호도가 존재하는 영화라면 사용자들에게 추천하면 좋을 것이라 판단된다.

### Studios
"""

# 각 데이터의 요소 리스트화 함수
def value_ls(df, col):
    ls = []
    
    for i in df[col]:
        if ',' in i:
            for j in i.split(', '):
                ls.append(j)
        else:
            ls.append(i)
    return ls

# 각 요소별 개수 확인 함수
def value_counting(U_value_ls, val_ls):
    counting = {i:0 for i in U_value_ls}
    for i in U_value_ls:
        counting[i] = val_ls.count(i)
#         if i == 'Unknown':
#             continue
#         else:
#             counting[i] = val_ls.count(i)
    return counting

Studios = list(set(value_ls(df_anime, 'Studios'))) # 장르 목록만들기(중복x)

# 데이터별 장르 리스트화
Studios1k = value_ls(top1000,'Studios') # 1k가 가진 장르들 리스트
Studios5k = value_ls(top5000,'Studios') # 5k가 가진 장르들 리스트
Studiosall = value_ls(df_anime,'Studios')

# 데이터별 장르 개수 확인
Studios1k = value_counting(Studios, Studios1k)
Studios5k = value_counting(Studios, Studios5k)
Studiosall = value_counting(Studios, Studiosall)

# 데이터 별 상위10개 장르
top10St_1k = sorted(Studios1k, key=Studios1k.get, reverse=True)[:5]
top10St_5k = sorted(Studios5k, key=Studios5k.get, reverse=True)[:5]
top10St_all = sorted(Studiosall, key=Studiosall.get, reverse=True)[:5]

# 데이터 별 상위 10개 장르 시각화
import matplotlib.pyplot as plt
fig = plt.figure(constrained_layout=True)
f = fig.subplots(3, 1, sharey=False, squeeze=False)

plt.suptitle('anime Studios')
f[0,0].bar(top10St_1k, [Studios1k[i] for i in top10St_1k])
f[1,0].bar(top10St_5k, [Studios5k[i] for i in top10St_5k])
f[2,0].bar(top10St_all, [Studiosall[i] for i in top10St_all])
plt.show()

"""#### Studios 시각화 분석
Studios는기획, 제작을 하는 스튜디오를 의미한다. 총 스튜디오의 개수는 608곳 이며, 상위 5곳을 분류해보았다. 해당 시각화를 통해 알 수 있는 것은
- 첫째 전체데이터는 해당 스튜디오가 제작한 애니의 숫자를 알 수 있다면, 이를 통해 1k, 5k에서는 제작한 애니 대비 성공적으로 유명해진 애니의 숫자를 알 수 있다. 따라서 전체데이터에서만 보이는 스튜디오를 제외하고 사용자에게 추천한다면 사용자가 해당 애니를 선호하는데 최소한의 제작에 대한 퀄리티를 보장할 수 있을 것이라 사료됩니다.

- 둘째 첫째에서 언급한것처럼 모든 데이터셋에서 중복적으로 보여지는 스튜디오 중 제작애니의 수가 적지만, 1k,5k에서 높은 순위를 가지고 있는 스튜디오가 애니의 질을 높게 제작하고 있음을 알 수 있다 따라서, ['J.C.Staff', 'Madhouse']에서 제작된 애니의 경우 타 스튜디오에서 제작된 애니보다 선호할 확률이 높을 것이라 분석된다.

### Licensors
"""

Licensors = list(set(value_ls(df_anime, 'Licensors'))) # 장르 목록만들기(중복x)

# 데이터별 장르 리스트화
Licensors1k = value_ls(top1000,'Licensors') # 1k가 가진 장르들 리스트
Licensors5k = value_ls(top5000,'Licensors') # 5k가 가진 장르들 리스트
Licensorsall = value_ls(df_anime,'Licensors')

# 데이터별 장르 개수 확인
Licensors1k = value_counting(Licensors, Licensors1k)
Licensors5k = value_counting(Licensors, Licensors5k)
Licensorsall = value_counting(Licensors, Licensorsall)

# 데이터 별 상위10개 장르
top10Li_1k = sorted(Licensors1k, key=Licensors1k.get, reverse=True)[:5]
top10Li_5k = sorted(Licensors5k, key=Licensors5k.get, reverse=True)[:5]
top10Li_all = sorted(Licensorsall, key=Licensorsall.get, reverse=True)[:5]

# 데이터 별 상위 10개 장르 시각화
import matplotlib.pyplot as plt
fig = plt.figure(constrained_layout=True)
f = fig.subplots(3, 1, sharey=False, squeeze=False)

plt.suptitle('anime Licensors')
f[0,0].bar(top10Li_1k, [Licensors1k[i] for i in top10Li_1k])
f[1,0].bar(top10Li_5k, [Licensors5k[i] for i in top10Li_5k])
f[2,0].bar(top10Li_all, [Licensorsall[i] for i in top10Li_all])
plt.show()

"""#### Licensors 시각화 분석
Licensors는 상표 등록된 재산권을 가지고 있는 라이선서(회사체)를 의미한다. 총 라이선서의 수는 78곳 이며, 해당 시각화를 통해 알 수 있는 점은
- 첫째 대형 라이선서가 대부분의 애니에 대한 라이선스를 가지고 있으며, 상위 데이터에서도 그러한 모습이 나타난느 것으로 보아 대형 라이선서의 애니 중 쌍위 데이터에 있는 애니를 추천하는 것이 보편적인 선호에 합당한 추천으로 사료된다.

- 둘째 전체데이터에서 보여지지 않는 라이선서가 1k,5k에서 중복적으로 나타난다면, 해당 라이선서는 선택적으로 흥행할 수 있는 애니의 라이선스를 가지려고 한다라는 점을 알 수 있다. 따라서, 사용자가 그 라이선스의 애니에 높은 점수를 주었거나 긍정적인 표시를 한경우 해당 라이선서의 애니를 추천하는 것이 대형 라이선서의 애니를 추천하는 것보다, 긍정적인 선호를 끌어낼 수 있음을 의미한다.

### Genres
"""

gennres = list(set(value_ls(df_anime, 'Genres'))) # 장르 목록만들기(중복x)

# 데이터별 장르 리스트화
genres1k = value_ls(top1000,'Genres') # 1k가 가진 장르들 리스트
genres5k = value_ls(top5000,'Genres') # 5k가 가진 장르들 리스트
genresall = value_ls(df_anime,'Genres')

# 데이터별 장르 개수 확인
genres1k = value_counting(gennres, genres1k)
genres5k = value_counting(gennres, genres5k)
genresall = value_counting(gennres, genresall)

# 데이터 별 상위10개 장르
top10Gr_1k = sorted(genres1k, key=genres1k.get, reverse=True)[:5]
top10Gr_5k = sorted(genres5k, key=genres5k.get, reverse=True)[:5]
top10Gr_all = sorted(genres5k, key=genres5k.get, reverse=True)[:5]

# 데이터 별 상위 10개 장르 시각화
import matplotlib.pyplot as plt
fig = plt.figure(constrained_layout=True)
f = fig.subplots(3, 1, sharey=False, squeeze=False)

plt.suptitle('anime genres')
f[0,0].bar(top10Gr_1k, [genres1k[i] for i in top10Gr_1k])
f[1,0].bar(top10Gr_5k, [genres5k[i] for i in top10Gr_5k])
f[2,0].bar(top10Gr_all, [genresall[i] for i in top10Gr_all])
plt.show()

"""#### 장르에 대한 분석
장르의 총 개수는 44개로 각 데이터별 상위 10개의 장르를 살펴본 결과, 
- 첫째 코미디와 액션을 포함하는 애니가 가장 인기가 많은 장르로 분석된다.
- 둘째 상위1000개의 애니에서는 로맨스와 판타지, 슈퍼내츄럴이 다음을 이어가지만 다른 데이터에서 판타지, 드라마, 로맨스가 이어지는 것으로 보아 본편적으로 로맨스와 판타지가 섞여있을 경우 사용자들의 선호도가 높을 수 있음을 알 수 있다.
"""

df_animelist.head()

# table = pd.pivot_table(df_animelist, 
#                        values='rating', index='anime',columns='user', 
#                        aggfunc=np.sum, fill_value=0)

# cross = pd.crosstab(df_animelist.user, df_animelist.anime, df_animelist.rating, aggfunc=np.sum)

"""# 모델링 - 사용자의 레이팅을 예측하는 모델"""

# k1_anime_ids = top5000['MAL_ID'].unique().tolist()
# df_k1_rating_C = df_rating_complete[df_rating_complete["anime_id"] == 1]
# for i in k1_anime_ids[1:]:
#     df_k1_rating_C.append(df_rating_complete[df_rating_complete["anime_id"] == i])

user_ids = df_rating_complete["user_id"].unique().tolist()
user2user_encoded = {x: i for i, x in enumerate(user_ids)}
user_encoded2user = {i: x for i, x in enumerate(user_ids)}
df_rating_complete["user"] = df_rating_complete["user_id"].map(user2user_encoded)
n_users = len(user2user_encoded)

anime_ids = df_rating_complete["anime_id"].unique().tolist()
anime2anime_encoded = {x: i for i, x in enumerate(anime_ids)}
anime_encoded2anime = {i: x for i, x in enumerate(anime_ids)}
df_rating_complete["anime"] = df_rating_complete["anime_id"].map(anime2anime_encoded)
n_animes = len(anime2anime_encoded)

# Split
from sklearn.model_selection import train_test_split
X = df_rating_complete[['user', 'anime']].values
y = df_rating_complete["rating"]

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=.2, random_state=1)
X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=.25, random_state=1)

print("train:", X_train.shape, y_train.shape)
print("val: ", X_val.shape, y_val.shape)
print("test:", X_test.shape, y_test.shape)

"""## DL"""

# pip install tensorflow

X_train_array = [X_train[:, 0], X_train[:, 1]]
X_val_array = [X_val[:, 0], X_val[:, 1]]
X_test_array = [X_test[:, 0], X_test[:, 1]]

import tensorflow as tf
from tensorflow import keras

from tensorflow.keras import layers 
from tensorflow.keras.models import Model
from tensorflow.keras.optimizers import Adam
from tensorflow.keras.layers import Add, Activation, Lambda, BatchNormalization, Concatenate, Dropout, Input, Embedding, Dot, Reshape, Dense, Flatten

"""### 모델빌드업"""

def RecommenderNet():
    embedding_size = 128
    
    # 양의 정수(인덱스)를 고정 크기의 조밀한 벡터로 바꿉니다
    user = Input(name = 'user', shape = [1])
    user_embedding = Embedding(name = 'user_embedding', 
                       input_dim = n_users, 
                       output_dim = embedding_size)(user)
    
    anime = Input(name = 'anime', shape = [1])
    anime_embedding = Embedding(name = 'anime_embedding',
                       input_dim = n_animes, 
                       output_dim = embedding_size)(anime)
    
    # 두 텐서의 샘플 간의 내적을 계산하는 레이어
    x = Dot(name = 'dot_product', normalize = True, axes = 2)([user_embedding, anime_embedding])
    # 입력을 평탄화함
    x = Flatten()(x)
    
    # 조밀하게 연결된 일반 NN 레이어(kernel가중치 행렬에 적용되는 제약 함수"he_normal")
    x = Dense(1, kernel_initializer='he_normal')(x)
    # 입력을 정규화
    x = BatchNormalization()(x)
    # 시그모이드 활성화 함수를 적용(5보다 작으면 sigmoid0, 크면 1)
    x = Activation("sigmoid")(x)
    
    # 훈련 및 추론 기능을 사용하여 레이어를 개체로 그룹화
    model = Model(inputs=[user, anime], outputs=x)
    model.compile(loss='binary_crossentropy', metrics=["mae", "mse"], optimizer='Adam')
    
    return model

model = RecommenderNet()
model.summary()

# Callbacks
from tensorflow.keras.callbacks import Callback, ModelCheckpoint, LearningRateScheduler, TensorBoard, EarlyStopping, ReduceLROnPlateau

start_lr = 0.00001
min_lr = 0.00001
max_lr = 0.00005
batch_size = 10000

rampup_epochs = 5
sustain_epochs = 0
exp_decay = .8

def lrfn(epoch):
    if epoch < rampup_epochs:
        return (max_lr - start_lr)/rampup_epochs * epoch + start_lr
    elif epoch < rampup_epochs + sustain_epochs:
        return max_lr
    else:
        return (max_lr - min_lr) * exp_decay**(epoch-rampup_epochs-sustain_epochs) + min_lr

# 업데이트된 학습률 값을 가져와 최적화기에 업데이트된 학습률을 적용
lr_callback = LearningRateScheduler(lambda epoch: lrfn(epoch), verbose=0)

checkpoint_filepath = './weights.h5'

# Keras 모델 또는 모델 가중치를 특정 빈도로 저장하기 위한 
# 가중치를 일정 간격으로 저장하므로 나중에 모델 또는 가중치를 로드하여 저장된 상태에서 학습을 계속하도록 함
model_checkpoints = ModelCheckpoint(filepath=checkpoint_filepath # 모델 파일을 저장할 경로
                                        save_weights_only=True   # True이면 모델의 가중치 아니면 전체 모델 저장
                                        monitor='val_loss',      # 모니터링할 메트릭 이름
                                        mode='min',              # 모니터가 val_loss인 경우 min으로 설정
                                        save_best_only=True,     # 모델이 "최상"으로 간주될 때만 저장
                                        n_jobs=-1)

# 학습을 통한 개선이 유의미하지 않으면 학습을 중지함 => 3번동안 val_loss가 min이면 중지하고 베스트를 복원함
early_stopping = EarlyStopping(patience = 3, monitor='val_loss', 
                               mode='min', restore_best_weights=True)

my_callbacks = [
    model_checkpoints,
    lr_callback,
    early_stopping,   
]

"""### 모델학습"""

# Model training
history = model.fit(
    x=X_train_array,
    y=y_train,
    batch_size=batch_size,
    epochs=20,
    verbose=1,
    validation_data=(X_val_array, y_val),
    callbacks=my_callbacks
)

# model.load_weights(checkpoint_filepath)

# Epoch 1/20
# 3458/3458 [==============================] - 1831s 529ms/step - loss: 0.6794 - mae: 7.0071 - mse: 52.0233 - val_loss: 0.5567 - val_mae: 7.0039 - val_mse: 51.9765
# Epoch 2/20
# 3458/3458 [==============================] - 1815s 525ms/step - loss: 0.2996 - mae: 6.9968 - mse: 51.8632 - val_loss: 0.1307 - val_mae: 6.9913 - val_mse: 51.7946
# Epoch 3/20
# 3458/3458 [==============================] - 1888s 546ms/step - loss: -0.3095 - mae: 6.9802 - mse: 51.5992 - val_loss: -0.5490 - val_mae: 6.9716 - val_mse: 51.4965
# Epoch 4/20
# 3458/3458 [==============================] - 1855s 536ms/step - loss: -1.2680 - mae: 6.9590 - mse: 51.2033 - val_loss: -1.7882 - val_mae: 6.9480 - val_mse: 51.0132
# Epoch 5/20
# 3458/3458 [==============================] - 1853s 536ms/step - loss: -2.7679 - mae: 6.9352 - mse: 50.6403 - val_loss: -3.3943 - val_mae: 6.9204 - val_mse: 50.4044
# Epoch 6/20
# 3458/3458 [==============================] - 1795s 519ms/step - loss: -4.3160 - mae: 6.9053 - mse: 50.0768 - val_loss: -4.8935 - val_mae: 6.8894 - val_mse: 49.8802
# Epoch 7/20
# 3458/3458 [==============================] - 1778s 514ms/step - loss: -5.6899 - mae: 6.8784 - mse: 49.6402 - val_loss: -6.0758 - val_mae: 6.8665 - val_mse: 49.5301
# Epoch 8/20
# 3458/3458 [==============================] - 1781s 515ms/step - loss: -6.8113 - mae: 6.8589 - mse: 49.3377 - val_loss: -7.0454 - val_mae: 6.8499 - val_mse: 49.2834
# Epoch 9/20
# 3458/3458 [==============================] - 1787s 517ms/step - loss: -7.7594 - mae: 6.8444 - mse: 49.1141 - val_loss: -7.8486 - val_mae: 6.8374 - val_mse: 49.1016
# Epoch 10/20
# 3458/3458 [==============================] - 1789s 517ms/step - loss: -8.5728 - mae: 6.8332 - mse: 48.9424 - val_loss: -8.5500 - val_mae: 6.8273 - val_mse: 48.9562
# Epoch 11/20
# 3458/3458 [==============================] - 1800s 520ms/step - loss: -9.2799 - mae: 6.8242 - mse: 48.8062 - val_loss: -9.1572 - val_mae: 6.8194 - val_mse: 48.8436
# Epoch 12/20
# 3458/3458 [==============================] - 1798s 520ms/step - loss: -9.9028 - mae: 6.8169 - mse: 48.6951 - val_loss: -9.7158 - val_mae: 6.8124 - val_mse: 48.7442
# Epoch 13/20
# 3458/3458 [==============================] - 1805s 522ms/step - loss: -10.4571 - mae: 6.8108 - mse: 48.6028 - val_loss: -10.1773 - val_mae: 6.8072 - val_mse: 48.6711
# Epoch 14/20
# 3458/3458 [==============================] - 1808s 523ms/step - loss: -10.9567 - mae: 6.8056 - mse: 48.5247 - val_loss: -10.6446 - val_mae: 6.8016 - val_mse: 48.5937
# Epoch 15/20
# 3458/3458 [==============================] - 1808s 523ms/step - loss: -11.4134 - mae: 6.8011 - mse: 48.4566 - val_loss: -11.0266 - val_mae: 6.7978 - val_mse: 48.5402
# Epoch 16/20
# 3458/3458 [==============================] - 1805s 522ms/step - loss: -11.8357 - mae: 6.7972 - mse: 48.3968 - val_loss: -11.4025 - val_mae: 6.7941 - val_mse: 48.4881
# Epoch 17/20
# 3458/3458 [==============================] - 1811s 524ms/step - loss: -12.2305 - mae: 6.7936 - mse: 48.3432 - val_loss: -11.7677 - val_mae: 6.7904 - val_mse: 48.4371
# Epoch 18/20
# 3458/3458 [==============================] - 1808s 523ms/step - loss: -12.6030 - mae: 6.7903 - mse: 48.2947 - val_loss: -12.0782 - val_mae: 6.7877 - val_mse: 48.3995
# Epoch 19/20
# 3458/3458 [==============================] - 1900s 549ms/step - loss: -12.9568 - mae: 6.7873 - mse: 48.2503 - val_loss: -12.4040 - val_mae: 6.7845 - val_mse: 48.3567
# Epoch 20/20
# 3458/3458 [==============================] - 1840s 532ms/step - loss: -13.2967 - mae: 6.7845 - mse: 48.2092 - val_loss: -12.7269 - val_mae: 6.7816 - val_mse: 48.3166

"""### 모델성능"""

# Commented out IPython magic to ensure Python compatibility.
#Training results
import matplotlib.pyplot as plt
# %matplotlib inline

plt.plot(history.history["loss"][0:-2])
plt.plot(history.history["val_loss"][0:-2])
plt.title("model loss")
plt.ylabel("loss")
plt.xlabel("epoch")
plt.legend(["train", "val"], loc="upper left")
plt.show()

history_test = model.evaluate(
    x=X_test_array,
    y=y_test,
    batch_size=batch_size,
    verbose=1,
    callbacks=my_callbacks
)

# 모델의 가중치를 저장하기
def extract_weights(name, model):
    weight_layer = model.get_layer(name)
    weights = weight_layer.get_weights()[0]
    weights = weights / np.linalg.norm(weights, axis = 1).reshape((-1, 1))
    return weights

anime_weights = extract_weights('anime_embedding', model)
user_weights = extract_weights('user_embedding', model)

"""# sypnopsis Base Recommendation"""

df_synopsis = pd.read_csv(data1, low_memory=True).replace("Unknown", np.nan)
# df_synopsis = df_synopsis.replace("Unknown", np.nan)
df_synopsis.head(2)

# 사용하려는 데이터 정리하기
def getAnimeName(anime_id):
    try:
        name = df_synopsis[df_synopsis.anime_id == anime_id].Name.values[0]
        if name is np.nan:
            name = df_synopsis[df_synopsis.anime_id == anime_id]["English name"].values[0]
    except:
        print('error')
    
    return name

df_synopsis['anime_id'] = df_synopsis['MAL_ID']
df_synopsis['Name'] = df_synopsis.anime_id.apply(lambda x: getAnimeName(x))

df_synopsis.sort_values(by=['Score'], 
               inplace=True,
               ascending=False, 
               kind='quicksort',
               na_position='last')

df_synopsis = df_synopsis[["anime_id", "Name", 
         "Score", "Genres", "Episodes",
         "Type", "Premiered", "Members"]]

df_synopsis.head(2)

def getAnimeFrame(anime): # 애니 이름/id 를 받아서 df에서 해당 row 불러오기
    if isinstance(anime, int):
        return df_synopsis[df_synopsis.anime_id == anime]
    if isinstance(anime, str):
        return df_synopsis[df_synopsis.Name == anime]

# df_anime_with_synopsis = pd.read_csv(data2)

def get_sypnopsis(anime): # 애니 이름/id 를 받아서 df에서 해당 애니 sypnopsis 불러오기
    if isinstance(anime, str):
        return df_anime_with_synopsis[df_anime_with_synopsis['Name'] == anime].sypnopsis.values[0]
    elif isinstance(anime, int):
        return df_anime_with_synopsis[df_anime_with_synopsis['MAL_ID'] == anime].sypnopsis.values[0]
    else:
        print(f"{anime} is not in df_anime_with_synopsis")
        return None

np.argsort(np.dot(anime_weights, anime_weights[0]))[:11]

pd.set_option("max_colwidth", None)

# 애니명을 인자로 받아 비슷한 애니를 추천하기
def find_similar_animes(name, n=10, return_dist=False, neg=False):
    try:
        index = getAnimeFrame(name).anime_id.values[0] # 애니명에 해당하는 애니를 찾아 애니ID를 반환
        encoded_index = anime2anime_encoded.get(index) # 애니ID를 인코딩한 값으로 변환하여 반환
        weights = anime_weights                        # 모델의 가중치값
        
        dists = np.dot(weights, weights[encoded_index])# 모델의 가중치와 가중치 중 해당 애니의 가중치의 내적을 반환 
        sorted_dists = np.argsort(dists)               # 배열을 정렬한 인덱스를 반환
        
        n = n + 1            
        
        if neg:
            closest = sorted_dists[:n]                 # 인접한 가중치들을 찾음
        else:
            closest = sorted_dists[-n:]

        print('animes closest to {}'.format(name))

        if return_dist:
            return dists, closest

        rindex = df_synopsis

        SimilarityArr = []

        for close in closest:
            decoded_id = anime_encoded2anime.get(close) # 인코딩값을 애니ID로 변환
            sypnopsis = get_sypnopsis(decoded_id)       # 이후 해당 애니ID의 정보를 모아서 df로 만듬
            anime_frame = getAnimeFrame(decoded_id)
            anime_name = anime_frame.Name.values[0]
            genre = anime_frame.Genres.values[0]
            similarity = dists[close]                   # 애니의 가중치를 인접도로 저장
            SimilarityArr.append({"anime_id": decoded_id, "name": anime_name,
                                  "similarity": similarity,"genre": genre,
                                  'sypnopsis': sypnopsis})

        Frame = pd.DataFrame(SimilarityArr).sort_values(by="similarity", ascending=False) # 인접도를 기준으로 정렬
        return Frame[Frame.anime_id != index].drop(['anime_id'], axis=1)

    except:
        print('{}!, Not Found in Anime list'.format(name))

find_similar_animes('Dragon Ball Z', n=5, neg=False)

"""# User Based Recommendation"""

ratings_per_user[ratings_per_user>500]

ratings_per_user = df_rating_complete.groupby('user_id').size # user_id 요소별 갯수를 값으로 시리즈 반환
random_user = ratings_per_user.sample(1, random_state=None).index[0] # user_id 임의적 선택(
print('> user_id:', random_user)

pd.set_option("max_colwidth", None) # 내용이 다보이도록 지정된 옵션의 값을 설정

# user_id를 입력받아서 비슷한 취향을 가진 유저를 반환함
def find_similar_users(item_input, n=10,return_dist=False, neg=False):
    try:
        index = item_input
        encoded_index = user2user_encoded.get(index) # user_id 인코딩값으로 변환
        weights = user_weights                       # 모델 가중치 가져옴
    
        dists = np.dot(weights, weights[encoded_index]) # 백터들의 내적 구함
        sorted_dists = np.argsort(dists)                # 내적(배열)을 나열함
        
        n = n + 1
        
        if neg:
            closest = sorted_dists[:n]                # 인접한 값 슬라이싱
        else:
            closest = sorted_dists[-n:]

        print('> users similar to #{}'.format(item_input))

        if return_dist:
            return dists, closest
        
        rindex = df_synopsis
        SimilarityArr = []
        for close in closest:
            similarity = dists[close]

            if isinstance(item_input, int):
                decoded_id = user_encoded2user.get(close) # 원래 user_id로 변환
                SimilarityArr.append({"similar_users": decoded_id, 
                                      "similarity": similarity})
        
        Frame = pd.DataFrame(SimilarityArr).sort_values(by="similarity", 
                                                        ascending=False)
        
        return Frame
    
    except:
        print('{}!, Not Found in User list'.format(name))

similar_users = find_similar_users(int(random_user), 
                                   n=5, 
                                   neg=False)

similar_users = similar_users[similar_users.similarity > 0.4]
similar_users = similar_users[similar_users.similar_users != random_user]
similar_users.head(5)

"""# User preferences"""

# pip install wordcloud

from wordcloud import WordCloud # 말풍선
from collections import defaultdict # 누락된 값을 제공하기 위해 팩토리 함수를 호출하는 딕셔너리 서브 클래스
import matplotlib.pyplot as plt

# 장르들을 받아서 빈도값을 기준으로 말풍선그리는 함수
def showWordCloud(all_genres):
    genres_cloud = WordCloud(width=700, height=400, 
                             background_color='white', 
                             colormap='gnuplot').generate_from_frequencies(all_genres)
    
    plt.figure(figsize=(10,8)) 
    plt.imshow(genres_cloud, interpolation='bilinear') # 2D 일반 래스터와 같은 이미지로 표시
    plt.axis('off')
    plt.show()

# 프레임을 받아서 장르별 최다빈도 장르를 리스트로 반환하는 함수
def getFavGenre(frame, plot=False):
        frame.dropna(inplace=False)
        all_genres = defaultdict(int)
        
        genres_list = []
        for genres in frame['Genres']:
            if isinstance(genres, str):
                for genre in genres.split(','):
                    genres_list.append(genre)
                    all_genres[genre.strip()] += 1    
        if plot:
            showWordCloud(all_genres)
        
        return genres_list

# user_id를 매개변수로 받아서 해당 유저의 기록 중 일정 등급이상의 기록을 가져와 시청한 애니의 장르들을 말풍선화 하고, 목록화 하는 함수
def get_user_preferences(user_id, plot=False, verbose=0):
    animes_watched_by_user = df_rating_complete[df_rating_complete.user_id==user_id]
    user_rating_percentile = np.percentile(animes_watched_by_user.rating, 75) # 배열 요소 의 75번째 백분위수를 반환
    animes_watched_by_user = animes_watched_by_user[animes_watched_by_user.rating >= user_rating_percentile]
    top_animes_user = (
        animes_watched_by_user.sort_values(by="rating", ascending=False)#.head(10)
        .anime_id.values
    )
    
    anime_df_rows = df_synopsis[df_synopsis["anime_id"].isin(top_animes_user)]
    anime_df_rows = anime_df_rows[["Name", "Genres"]]
    
    if verbose != 0:
        print("> User #{} has rated {} movies (avg. rating = {:.1f})".format(
          user_id, len(animes_watched_by_user),
          animes_watched_by_user['rating'].mean(),
        ))
    
        print('> preferred genres')
    
    if plot:
        getFavGenre(anime_df_rows, plot)
        
    return anime_df_rows#.eng_version.values

user_pref = get_user_preferences(random_user, plot=True, verbose=1)
print('> animes highly rated by this user')

pd.DataFrame(user_pref).head(5)

# 비슷한 취향의 유저들을 매개변수로 받아 해당 유저들의 정보를 기반으로 장르말풍선, 선호애니 리스트를 반환하는 함수
def get_recommended_animes(similar_users, n=10):
    recommended_animes = []
    anime_list = []
    
    for user_id in similar_users.similar_users.values:
        pref_list = get_user_preferences(int(user_id), verbose=0)
        pref_list = pref_list[~ pref_list.Name.isin(user_pref.Name.values)]
        anime_list.append(pref_list.Name.values)
        
    anime_list = pd.DataFrame(anime_list)
    sorted_list = pd.DataFrame(pd.Series(anime_list.values.ravel()).value_counts()).head(n)
    
    for i, anime_name in enumerate(sorted_list.index):        
        n_user_pref = sorted_list[sorted_list.index == anime_name].values[0][0]
        if isinstance(anime_name, str):
            try:
                frame = getAnimeFrame(anime_name)
                anime_id = frame.anime_id.values[0]
                genre = frame.Genres.values[0]
                sypnopsis = get_sypnopsis(int(anime_id))
                recommended_animes.append({#"anime_id": anime_id ,
                                            "n": n_user_pref,
                                            "anime_name": anime_name, 
                                            "Genres": genre, 
                                            "sypnopsis": sypnopsis})
            except:
                pass
    
    return pd.DataFrame(recommended_animes)

recommended_animes = get_recommended_animes(similar_users, n=10)
getFavGenre(recommended_animes, plot=True)

print('\n> Top recommendations for user: {}'.format(random_user))
recommended_animes

"""# Ranking based Recommendation"""

print("Showing recommendations for user: {}".format(random_user))
print("===" * 25)

animes_watched_by_user = df_rating_complete[df_rating_complete.user_id==random_user]
anime_not_watched_df = df_synopsis[
    ~df_synopsis["anime_id"].isin(animes_watched_by_user.anime_id.values)
]

# 앞의 set과 뒤의 set모두 존재하는 값을 리스트로 반환
anime_not_watched = list(
    set(anime_not_watched_df['anime_id']).intersection(set(anime2anime_encoded.keys()))
)

# user_id 인코딩
anime_not_watched = [[anime2anime_encoded.get(x)] for x in anime_not_watched] 
user_encoder = user2user_encoded.get(random_user)


user_anime_array = np.hstack( # 배열을 수평으로 순서대로 쌓음
    ([[user_encoder]] * len(anime_not_watched), anime_not_watched)
)

user_anime_array = [user_anime_array[:, 0], user_anime_array[:, 1]]

ratings = model.predict(user_anime_array).flatten() # 모델을 통해 안본 애니들 추천예상

top_ratings_indices = (-ratings).argsort()[:10] #배열을 정렬할 인덱스를 슬라이싱해서 반환

recommended_anime_ids = [ # 상위 레이팅 안본 애니들 애니ID 리스트로 반환함
    anime_encoded2anime.get(anime_not_watched[x][0]) for x in top_ratings_indices
]

Results = []
top_rated_ids = []

for index, anime_id in enumerate(anime_not_watched):
    rating = ratings[index]
    id_ = anime_encoded2anime.get(anime_id[0])
    
    if id_ in recommended_anime_ids:
        top_rated_ids.append(id_)
        try:
            condition = (df_synopsis.anime_id == id_)
            name = df_synopsis[condition]['Name'].values[0]
            genre = df_synopsis[condition].Genres.values[0]
            score = df_synopsis[condition].Score.values[0]
            sypnopsis = get_sypnopsis(int(id_))
            
        except:
            continue
        Results.append({#"anime_id": id_, 
                        "name": name, 
                        "pred_rating": rating,
                        "genre": genre, 
                        'sypnopsis': sypnopsis})

print("---" * 25)
print("> Top 10 anime recommendations")
print("---" * 25)

Results = pd.DataFrame(Results).sort_values(by='pred_rating', ascending=False)
Results

